{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "from smart_open import smart_open\n",
    "import msgpack\n",
    "\n",
    "\n",
    "class GeneratorTextsXML:\n",
    "    def __init__(self, dir_path):\n",
    "        self._dir_path = dir_path\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self._dir_path):\n",
    "            print(fname)\n",
    "            file_path = os.path.join(self._dir_path, fname)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "            with open(file_path, 'rb') as f:\n",
    "                for event, elem in ET.iterparse(f, html=True, tag='doc', encoding='utf8'):\n",
    "                    if elem.text is not None:\n",
    "                        yield elem.text\n",
    "                        \n",
    "\n",
    "class GeneratorTextsJSON:\n",
    "    def __init__(self, dir_path):\n",
    "        self._dir_path = dir_path\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self._dir_path):\n",
    "            print(fname)\n",
    "            file_path = os.path.join(self._dir_path, fname)\n",
    "            if not os.path.isfile(file_path):\n",
    "                continue\n",
    "                \n",
    "            with smart_open(file_path, 'r', encoding='utf8') as f:\n",
    "                for line in f:\n",
    "                    yield json.loads(line)['text']\n",
    "                    \n",
    "                    \n",
    "def parse_wiki(gen, output_path, chunk_size):\n",
    "    chunk_iter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            chunk = []\n",
    "            for _ in range(chunk_size):\n",
    "                chunk.append(next(gen_iter))\n",
    "        except StopIteration as e:\n",
    "            break\n",
    "        finally:\n",
    "            chunk_iter += 1\n",
    "            print('#Iter: ', chunk_iter)\n",
    "\n",
    "            res = ppl(chunk)\n",
    "\n",
    "            file_path = os.path.join(output_path, str(chunk_iter) + '.pckl')\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(msgpack.packb([e['lemma'] for e in res], use_bin_type=True))\n",
    "                \n",
    "                \n",
    "                \n",
    "def test_extracted_files(data_path, start, plus):\n",
    "    for num, e in enumerate(GeneratorTexts(data_path)):\n",
    "        print(num)\n",
    "        if num >= start and num < start + plus + 1:\n",
    "            print('============')\n",
    "            print(e)\n",
    "\n",
    "        if num == start + 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.processor_sentence_splitter import ProcessorSentenceSplitter\n",
    "from isanlp.ru.processor_tokenizer_ru import ProcessorTokenizerRu\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.wrapper_multi_process_document import WrapperMultiProcessDocument\n",
    "\n",
    "\n",
    "ppl_single_creator = lambda : PipelineCommon([\n",
    "    (ProcessorTokenizerRu(), ['text'], {0 : 'tokens'}),\n",
    "    (ProcessorSentenceSplitter(), ['tokens'], {0 : 'sentences'}),\n",
    "    (ProcessorMystem(), ['tokens', 'sentences'], {'lemma' : 'lemma'})\n",
    "])\n",
    "\n",
    "ppl = WrapperMultiProcessDocument([ppl_single_creator() for _ in range(10)], progress_bar=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/notebook/projects/compounds/workdir/extracted/AA/'\n",
    "gen = GeneratorTexts(data_path)\n",
    "gen_iter = iter(gen)\n",
    "\n",
    "output_path = '/notebook/projects/compounds/workdir/parse/'\n",
    "chunk_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_02\n",
      "#Iter:  1\n",
      "#Iter:  2\n",
      "#Iter:  3\n",
      "#Iter:  4\n",
      "#Iter:  5\n",
      "#Iter:  6\n",
      "#Iter:  7\n",
      "#Iter:  8\n",
      "#Iter:  9\n",
      "#Iter:  10\n",
      "#Iter:  11\n",
      "#Iter:  12\n",
      "#Iter:  13\n",
      "#Iter:  14\n",
      "#Iter:  15\n",
      "#Iter:  16\n",
      "wiki_03\n",
      "#Iter:  17\n",
      "#Iter:  18\n",
      "#Iter:  19\n",
      "#Iter:  20\n",
      "#Iter:  21\n",
      "#Iter:  22\n",
      "#Iter:  23\n",
      "#Iter:  24\n",
      "#Iter:  25\n",
      "#Iter:  26\n",
      "#Iter:  27\n",
      "#Iter:  28\n",
      "#Iter:  29\n",
      "#Iter:  30\n",
      "#Iter:  31\n",
      "#Iter:  32\n",
      "#Iter:  33\n",
      "#Iter:  34\n",
      "wiki_04\n",
      "#Iter:  35\n",
      "#Iter:  36\n",
      "#Iter:  37\n",
      "#Iter:  38\n",
      "#Iter:  39\n",
      "#Iter:  40\n",
      "#Iter:  41\n",
      "#Iter:  42\n",
      "#Iter:  43\n",
      "#Iter:  44\n",
      "#Iter:  45\n",
      "#Iter:  46\n",
      "#Iter:  47\n",
      "#Iter:  48\n",
      "#Iter:  49\n",
      "#Iter:  50\n",
      "wiki_05\n",
      "#Iter:  51\n",
      "#Iter:  52\n",
      "#Iter:  53\n",
      "#Iter:  54\n",
      "#Iter:  55\n",
      "#Iter:  56\n",
      "wiki_01\n",
      "#Iter:  57\n",
      "#Iter:  58\n",
      "#Iter:  59\n",
      "#Iter:  60\n",
      "#Iter:  61\n",
      "#Iter:  62\n",
      "#Iter:  63\n",
      "#Iter:  64\n",
      "#Iter:  65\n",
      "#Iter:  66\n",
      "#Iter:  67\n",
      "#Iter:  68\n",
      "#Iter:  69\n",
      "wiki_00\n",
      "#Iter:  70\n",
      "#Iter:  71\n",
      "#Iter:  72\n",
      "#Iter:  73\n",
      "#Iter:  74\n",
      "#Iter:  75\n",
      "#Iter:  76\n",
      "#Iter:  77\n",
      "#Iter:  78\n"
     ]
    }
   ],
   "source": [
    "data_path = '/notebook/projects/compounds/workdir/wiki_json/AA'\n",
    "gen = GeneratorTextsJSON(data_path)\n",
    "gen_iter = iter(gen)\n",
    "\n",
    "output_path = '/notebook/projects/compounds/workdir/parse5/'\n",
    "chunk_size = 20000\n",
    "parse_wiki(gen, output_path, chunk_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
