{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./workdir/annotation_small.csv')\n",
    "dataset[dataset['Катя (short list)'].isin({1.0, 0.0})].to_csv('./workdir/annotation_small_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "loaded\n",
      "1060891\n",
      "Starting lemmatize\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from argparse import ArgumentParser\n",
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm\n",
    "from gensim.models import FastText\n",
    "\n",
    "morph = Mystem()\n",
    "\n",
    "model_path = './workdir/models/model_fast2vec_300_5'\n",
    "vectors_path = './workdir/vectors.txt'\n",
    "#compounds_path = './workdir/annotation_katya_selected.csv'\n",
    "compounds_path = './workdir/annotation_small_selected.csv'\n",
    "\n",
    "comp = pd.read_csv(compounds_path)\n",
    "\n",
    "chast1 = list(comp['Часть 1'].values)\n",
    "chast2 = list(comp['Часть 2'].values)\n",
    "\n",
    "print('loading model')\n",
    "model = FastText.load(model_path)\n",
    "print('loaded')\n",
    "print(len(model.wv.vocab))\n",
    "\n",
    "part1_lem = []\n",
    "part2_lem = []\n",
    "\n",
    "print('Starting lemmatize')\n",
    "\n",
    "for w1, w2 in zip(chast1, chast2):\n",
    "    lem_w1 = morph.lemmatize(w1)[0]\n",
    "    lem_w2 = morph.lemmatize(w2)[0]\n",
    "    part1_lem.append(lem_w1)\n",
    "    part2_lem.append(lem_w2)\n",
    "\n",
    "with open(vectors_path, 'w') as vectors:\n",
    "    for w1, w2 in zip(part1_lem, part2_lem):\n",
    "        if w1 in model.wv.vocab:\n",
    "            vectors.write(w1 + ' ' + ' '.join(model.wv[w1].astype('str')) + '\\r\\n')\n",
    "        else:\n",
    "            print(w1)\n",
    "        if w2 in model.wv.vocab:\n",
    "            vectors.write(w2 + ' ' + ' '.join(model.wv[w2].astype('str')) + '\\r\\n')\n",
    "        else:\n",
    "            print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "loaded\n",
      "1060891\n",
      "Starting lemmatize\n",
      "приговор_судно\n",
      "сектор_мишень\n",
      "трагический_плач\n",
      "красный_черт\n",
      "неблагодарный_дело\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from argparse import ArgumentParser\n",
    "from pymystem3 import Mystem\n",
    "from tqdm import tqdm\n",
    "from gensim.models import FastText\n",
    "\n",
    "morph = Mystem()\n",
    "\n",
    "model_path = './workdir/models/model_fast2vec_300_5'\n",
    "vecotr_path = './workdir/vectors_compounds.txt'\n",
    "#compounds_path = './workdir/annotation_katya_selected.csv'\n",
    "compounds_path = './workdir/annotation_small_selected.csv'\n",
    "\n",
    "\n",
    "comp = pd.read_csv(compounds_path)\n",
    "\n",
    "chast1 = list(comp['Часть 1'].values)\n",
    "chast2 = list(comp['Часть 2'].values)\n",
    "\n",
    "print('loading model')\n",
    "model = FastText.load(model_path)\n",
    "print('loaded')\n",
    "print(len(model.wv.vocab))\n",
    "\n",
    "comp_lem = []\n",
    "\n",
    "print('Starting lemmatize')\n",
    "\n",
    "for w1, w2 in zip(chast1, chast2):\n",
    "    lem_w1 = morph.lemmatize(w1)[0]\n",
    "    lem_w2 = morph.lemmatize(w2)[0]\n",
    "    comp_lem.append(lem_w1 + '_' + lem_w2)\n",
    "\n",
    "with open(vecotr_path, 'w') as vectors:\n",
    "    for w1 in comp_lem:\n",
    "        if w1 in model.wv.vocab:\n",
    "            vectors.write(w1 + ' ' + ' '.join(model.wv[w1].astype('str')) + '\\r\\n')\n",
    "        else:\n",
    "            print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "201\n",
      "201\n",
      "196 examples retrieved for experiment\n",
      "0.3952770772648245\n",
      "-0.31842107787166385\n",
      "-0.32484633827109205\n",
      "-0.32188083347135593\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial import distance\n",
    "from pymystem3 import Mystem\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "#compounds_path = './workdir/annotation_katya_selected.csv'\n",
    "compounds_path = './workdir/annotation_small_selected.csv'\n",
    "\n",
    "morph = Mystem()\n",
    "\n",
    "def acquiring(wordvecs, compvecs):\n",
    "    comp = pd.read_csv(compounds_path)\n",
    "\n",
    "    part1 = list(comp['Часть 1'].values)\n",
    "    part2 = list(comp['Часть 2'].values)\n",
    "    values = list(comp['Катя (short list)'].values)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    compounds = []\n",
    "    classes = []\n",
    "\n",
    "    for w1, w2, v in zip(part1, part2, values):\n",
    "        if v != 2:\n",
    "            lem_w1 = morph.lemmatize(w1)[0]\n",
    "            lem_w2 = morph.lemmatize(w2)[0]\n",
    "            compounds.append('_'.join([lem_w1, lem_w2]))\n",
    "            classes.append(v)\n",
    "\n",
    "    vecs1 = []\n",
    "    vecs2 = []\n",
    "    vecsc = []\n",
    "    true_comp_class = []\n",
    "\n",
    "    words = []\n",
    "    comps = []\n",
    "    \n",
    "    with open(wordvecs) as w:\n",
    "        for line in w:\n",
    "            words.append(line.split())\n",
    "\n",
    "    with open(compvecs) as c:\n",
    "        for line in c:\n",
    "            comps.append(line.split())\n",
    "\n",
    "    print(len(compounds))\n",
    "    print(len(values))\n",
    "    for compound, value in zip(compounds, values):\n",
    "        comp_flag = 0\n",
    "        w1_flag = 0\n",
    "        w2_flag = 0\n",
    "        for line in comps:\n",
    "            if compound == line[0]:\n",
    "                vecc = np.array(line[1:]).astype(np.float32)\n",
    "                comp_flag = 1\n",
    "        w1 = compound.split('_')[0]\n",
    "        w2 = compound.split('_')[1]\n",
    "        if comp_flag:\n",
    "            for line in words:\n",
    "                if w1 == line[0]:\n",
    "                    vec1 = np.array(line[1:]).astype(np.float32)\n",
    "                    w1_flag = 1\n",
    "                    break\n",
    "            for line in words:\n",
    "                if w2 == line[0]:\n",
    "                    w2_flag = 1\n",
    "                    vec2 = np.array(line[1:]).astype(np.float32)\n",
    "                    break\n",
    "        if comp_flag and w1_flag and w2_flag:\n",
    "            vecs1.append(vec1)\n",
    "            vecs2.append(vec2)\n",
    "            vecsc.append(vecc)\n",
    "            true_comp_class.append(value)\n",
    "\n",
    "    print(len(vecsc), 'examples retrieved for experiment')\n",
    "    return vecs1, vecs2, vecsc, true_comp_class\n",
    "\n",
    "\n",
    "def get_mean(part1_vecs, part2_vecs):\n",
    "    parts_mean = []\n",
    "    for vec1, vec2 in zip(part1_vecs, part2_vecs):\n",
    "        parts_mean.append((vec1 + vec2) / 2)\n",
    "\n",
    "    return parts_mean\n",
    "\n",
    "\n",
    "def cosine_between_parts_and_compound(part1_vecs, part2_vecs, comp_vecs, true_class):\n",
    "    parts_mean = get_mean(part1_vecs, part2_vecs)\n",
    "\n",
    "    cosines = []\n",
    "    for w, c in zip(parts_mean, comp_vecs):\n",
    "        cosines.append(abs(1 - distance.cosine(w, c)))\n",
    "    \n",
    "    print(spearmanr(cosines, true_class)[0])\n",
    "\n",
    "\n",
    "def chebyshev_between_parts_and_compound(part1_vecs, part2_vecs, comp_vecs, true_class):\n",
    "    parts_mean = get_mean(part1_vecs, part2_vecs)\n",
    "\n",
    "    chebyshevs = []\n",
    "    for w, c in zip(parts_mean, comp_vecs):\n",
    "        chebyshevs.append(abs(distance.chebyshev(w, c)))\n",
    "\n",
    "    print(spearmanr(chebyshevs, true_class)[0])\n",
    "\n",
    "\n",
    "def manhattan_between_parts_and_compound(part1_vecs, part2_vecs, comp_vecs, true_class):\n",
    "    parts_mean= get_mean(part1_vecs, part2_vecs)\n",
    "\n",
    "    manhattans = []\n",
    "    for w, c in zip(parts_mean, comp_vecs):\n",
    "        manhattans.append(abs(distance.cityblock(w, c)))\n",
    "    print(spearmanr(manhattans, true_class)[0])\n",
    "\n",
    "\n",
    "def euclidean_between_parts_and_compound(part1_vecs, part2_vecs, comp_vecs, true_class):\n",
    "    parts_mean= get_mean(part1_vecs, part2_vecs)\n",
    "\n",
    "    euclideans = []\n",
    "    for w, c in zip(parts_mean, comp_vecs):\n",
    "        euclideans.append(abs(distance.euclidean(w, c)))\n",
    "    print(spearmanr(euclideans, true_class)[0])\n",
    "\n",
    "\n",
    "vectors_parts_path = './workdir/vectors.txt'\n",
    "vectors_compounds_path = './workdir/vectors_compounds.txt'\n",
    "\n",
    "w1, w2, c, true = acquiring(vectors_parts_path, vectors_compounds_path)\n",
    "\n",
    "cosine_between_parts_and_compound(w1, w2, c, true)\n",
    "chebyshev_between_parts_and_compound(w1, w2, c, true)\n",
    "manhattan_between_parts_and_compound(w1, w2, c, true)\n",
    "euclidean_between_parts_and_compound(w1, w2, c, true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
